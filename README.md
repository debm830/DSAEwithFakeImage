# DSAE with Fake Image Generation

Remote sensing scene classification is a fundamental task in remote sensing image analysis. It aims to assign a semantic label to a given image based on its content, such as land cover type or land use. However, remote sensing scene classification faces a significant challenge of limited labelled data due to the high cost and time-consuming nature of acquiring ground truth labels. This limitation hinders the development of accurate classification models and makes it challenging to apply these models to new unseen scenes.
To address this challenge, recent studies have proposed zero-shot learning approaches for remote sensing scene classification. Zero-shot learning aims to classify images of unseen scenes by utilizing semantic information from labelled scenes. However, these methods often rely on strong assumptions about the semantic relationships between labelled and unseen scenes, which may not always hold in practice.
In this paper, we propose a novel approach for zero-shot remote sensing scene classification that leverages a distance-constrained semantic autoencoder and a fake image generator. The approach aims to learn a low-dimensional feature representation that captures the semantic information of the scene and augment the limited labelled data by generating synthetic images of seen and unseen scenes. Specifically, the distance-constrained semantic autoencoder learns a feature representation that preserves the semantic similarities between labelled scenes while also enforcing a distance constraint between labelled and unseen scenes. The fake image generator, on the other hand, generates synthetic images of seen and unseen scenes by leveraging the learned feature representation.
[Image Extractor Link : https://colab.research.google.com/drive/1H21WZ-zW1x834sUi7WbPBYFo2KHpT9IX?usp=share_link ]
